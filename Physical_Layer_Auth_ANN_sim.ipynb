{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Physical_Layer_Auth_ANN_sim.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2ez6BifG9gwt8IoBV7Qmb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfsaneHeydari/AutoEncoder-based-communication-system-simulation/blob/main/Physical_Layer_Auth_ANN_sim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGPELfTVXgL5"
      },
      "source": [
        "simulation of\n",
        "A Learning Approach for Physical Layer\n",
        "Authentication Using Adaptive\n",
        "Neural Network paper "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-1xeMAbX1iM"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Input, Dense, GaussianNoise,Lambda,Dropout\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam,SGD\n",
        "from keras import backend as K\n",
        "import math\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eUI_nk7YDOd"
      },
      "source": [
        "class End2EndCommunicationSys():\n",
        "  \n",
        "  def __init__(self, signal_size, channel_number ):\n",
        "\n",
        "    self.communication_sys = None\n",
        "    self.transmiter = None\n",
        "    self.receiver = None\n",
        "    self.channel_layer = None\n",
        "    self.receivedSignals = None\n",
        "    self.data = (self.getData(signal_size))\n",
        "\n",
        "    self.generateModel(signal_size, channel_number)\n",
        "    \n",
        "\n",
        "  def generateModel(self,signal_size, channel_number):\n",
        "\n",
        "    k = np.log2(signal_size)  \n",
        "    k = int(k)\n",
        "    R = k/channel_number\n",
        "\n",
        "    input_signal = Input(shape=(signal_size,))\n",
        "    encoded = Dense(signal_size, activation='sigmoid')(input_signal) #512 for signal size\n",
        "    # encoded1 = Dense((signal_size/4), activation='tanh')(encoded) # 128\n",
        "    encoded2 = Dense(channel_number, activation='linear')(encoded) # 16\n",
        "    encoded3 = Lambda(lambda x: np.sqrt(channel_number)*K.l2_normalize(x,axis=1))(encoded2) # channel number is 16\n",
        "\n",
        "    EbNo_train = 5.01187 #  coverted 7 db of EbNo\n",
        "    self.channel_layer = GaussianNoise(1)(encoded3)\n",
        "\n",
        "    # decoded = Dense((signal_size/4), activation='tanh')(self.channel_layer)\n",
        "    # decoded = Dense(signal_size, activation='relu')(self.channel_layer)\n",
        "    decoded2 = Dense(signal_size, activation='linear')(self.channel_layer)\n",
        "    self.communication_sys = Model(input_signal, decoded2)\n",
        "    # adam = Adam(lr=0.00001)\n",
        "    # adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.005,\n",
        "    decay_steps=50000,\n",
        "    decay_rate=0.9)\n",
        "\n",
        "    adam = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    self.communication_sys.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy','mse'])\n",
        "\n",
        "# categorical_crossentropy\n",
        "    data = self.data\n",
        "    # print('data = {}'.format(data))\n",
        "    self.communication_sys.fit(data, data,\n",
        "                epochs=250,\n",
        "                batch_size=32,\n",
        "                validation_split=0.1)\n",
        "    \n",
        "    self.transmiter = Model(input_signal, encoded3)\n",
        "    self.receivedSignals = Model(input_signal, self.channel_layer)\n",
        "  \n",
        "    encoded_input = Input(shape=(channel_number,))\n",
        "\n",
        "    # deco = self.communication_sys.layers[-3](encoded_input)\n",
        "    deco = self.communication_sys.layers[-2](encoded_input)\n",
        "    deco = self.communication_sys.layers[-1](deco)\n",
        "    self.receiver = Model(encoded_input, deco)\n",
        "\n",
        "\n",
        "  def getData(self, signal_size):\n",
        "\n",
        "    N = 10000\n",
        "    data = []\n",
        "  #  l = np.random.randn(signal_size,size=N)\n",
        "    # print(l)\n",
        "\n",
        "    for i in range(N):\n",
        "        temp = np.random.randn(signal_size)\n",
        "        # print (temp)\n",
        "        # for i in range(temp.shape[1]):\n",
        "        #   if temp[0,i]>=0.3:\n",
        "        #     temp[0,i] = 1\n",
        "        #   else:\n",
        "        #     temp[0,i] = 0\n",
        "      #  temp[0,l[i]] = 1\n",
        "        \n",
        "        data.append(temp)\n",
        "\n",
        "    data = np.array(data)\n",
        "    return data\n",
        "\n",
        "\n",
        "  # def getRss(self,received_data):\n",
        "\n",
        "  #   rss = np.zeros((1,256))\n",
        "  #   i = 0\n",
        "  #   for d in received_data:\n",
        "  #     pr = d*d\n",
        "  #     rss[0,i]=(10*(math.log(pr)))\n",
        "  #     i += 1\n",
        "  #     print(rss[i])\n",
        "\n",
        "    # pr = pr/len(received_data)\n",
        "\n",
        "    # return rss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qvNYScBC_dO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "800beaa0-3836-45b2-a53c-56872398fd4d"
      },
      "source": [
        "comSys = End2EndCommunicationSys(512,16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 1.0070 - accuracy: 0.0044 - mse: 1.0070 - val_loss: 0.9804 - val_accuracy: 0.0050 - val_mse: 0.9804\n",
            "Epoch 2/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9935 - accuracy: 0.0042 - mse: 0.9935 - val_loss: 0.9789 - val_accuracy: 0.0070 - val_mse: 0.9789\n",
            "Epoch 3/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9921 - accuracy: 0.0047 - mse: 0.9921 - val_loss: 0.9787 - val_accuracy: 0.0110 - val_mse: 0.9787\n",
            "Epoch 4/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9915 - accuracy: 0.0047 - mse: 0.9915 - val_loss: 0.9777 - val_accuracy: 0.0050 - val_mse: 0.9777\n",
            "Epoch 5/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9914 - accuracy: 0.0047 - mse: 0.9914 - val_loss: 0.9785 - val_accuracy: 0.0060 - val_mse: 0.9785\n",
            "Epoch 6/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9913 - accuracy: 0.0050 - mse: 0.9913 - val_loss: 0.9782 - val_accuracy: 0.0080 - val_mse: 0.9782\n",
            "Epoch 7/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9910 - accuracy: 0.0047 - mse: 0.9910 - val_loss: 0.9778 - val_accuracy: 0.0070 - val_mse: 0.9778\n",
            "Epoch 8/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9908 - accuracy: 0.0046 - mse: 0.9908 - val_loss: 0.9776 - val_accuracy: 0.0050 - val_mse: 0.9776\n",
            "Epoch 9/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9908 - accuracy: 0.0057 - mse: 0.9908 - val_loss: 0.9774 - val_accuracy: 0.0010 - val_mse: 0.9774\n",
            "Epoch 10/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9907 - accuracy: 0.0052 - mse: 0.9907 - val_loss: 0.9776 - val_accuracy: 0.0040 - val_mse: 0.9776\n",
            "Epoch 11/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9904 - accuracy: 0.0043 - mse: 0.9904 - val_loss: 0.9773 - val_accuracy: 0.0080 - val_mse: 0.9773\n",
            "Epoch 12/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9905 - accuracy: 0.0051 - mse: 0.9905 - val_loss: 0.9777 - val_accuracy: 0.0060 - val_mse: 0.9777\n",
            "Epoch 13/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9904 - accuracy: 0.0038 - mse: 0.9904 - val_loss: 0.9774 - val_accuracy: 0.0070 - val_mse: 0.9774\n",
            "Epoch 14/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9904 - accuracy: 0.0047 - mse: 0.9904 - val_loss: 0.9775 - val_accuracy: 0.0040 - val_mse: 0.9775\n",
            "Epoch 15/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9904 - accuracy: 0.0050 - mse: 0.9904 - val_loss: 0.9774 - val_accuracy: 0.0040 - val_mse: 0.9774\n",
            "Epoch 16/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9904 - accuracy: 0.0044 - mse: 0.9904 - val_loss: 0.9772 - val_accuracy: 0.0080 - val_mse: 0.9772\n",
            "Epoch 17/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9903 - accuracy: 0.0049 - mse: 0.9903 - val_loss: 0.9770 - val_accuracy: 0.0040 - val_mse: 0.9770\n",
            "Epoch 18/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9902 - accuracy: 0.0044 - mse: 0.9902 - val_loss: 0.9772 - val_accuracy: 0.0080 - val_mse: 0.9772\n",
            "Epoch 19/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9902 - accuracy: 0.0052 - mse: 0.9902 - val_loss: 0.9771 - val_accuracy: 0.0080 - val_mse: 0.9771\n",
            "Epoch 20/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9902 - accuracy: 0.0057 - mse: 0.9902 - val_loss: 0.9773 - val_accuracy: 0.0080 - val_mse: 0.9773\n",
            "Epoch 21/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9902 - accuracy: 0.0030 - mse: 0.9902 - val_loss: 0.9773 - val_accuracy: 0.0110 - val_mse: 0.9773\n",
            "Epoch 22/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9901 - accuracy: 0.0057 - mse: 0.9901 - val_loss: 0.9771 - val_accuracy: 0.0040 - val_mse: 0.9771\n",
            "Epoch 23/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9901 - accuracy: 0.0044 - mse: 0.9901 - val_loss: 0.9771 - val_accuracy: 0.0080 - val_mse: 0.9771\n",
            "Epoch 24/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9900 - accuracy: 0.0046 - mse: 0.9900 - val_loss: 0.9773 - val_accuracy: 0.0040 - val_mse: 0.9773\n",
            "Epoch 25/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9900 - accuracy: 0.0046 - mse: 0.9900 - val_loss: 0.9773 - val_accuracy: 0.0060 - val_mse: 0.9773\n",
            "Epoch 26/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9901 - accuracy: 0.0042 - mse: 0.9901 - val_loss: 0.9769 - val_accuracy: 0.0090 - val_mse: 0.9769\n",
            "Epoch 27/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9900 - accuracy: 0.0058 - mse: 0.9900 - val_loss: 0.9770 - val_accuracy: 0.0050 - val_mse: 0.9770\n",
            "Epoch 28/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9900 - accuracy: 0.0038 - mse: 0.9900 - val_loss: 0.9769 - val_accuracy: 0.0040 - val_mse: 0.9769\n",
            "Epoch 29/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9902 - accuracy: 0.0052 - mse: 0.9902 - val_loss: 0.9766 - val_accuracy: 0.0110 - val_mse: 0.9766\n",
            "Epoch 30/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9899 - accuracy: 0.0063 - mse: 0.9899 - val_loss: 0.9766 - val_accuracy: 0.0080 - val_mse: 0.9766\n",
            "Epoch 31/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9899 - accuracy: 0.0057 - mse: 0.9899 - val_loss: 0.9767 - val_accuracy: 0.0050 - val_mse: 0.9767\n",
            "Epoch 32/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9899 - accuracy: 0.0049 - mse: 0.9899 - val_loss: 0.9769 - val_accuracy: 0.0030 - val_mse: 0.9769\n",
            "Epoch 33/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9901 - accuracy: 0.0054 - mse: 0.9901 - val_loss: 0.9772 - val_accuracy: 0.0020 - val_mse: 0.9772\n",
            "Epoch 34/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9899 - accuracy: 0.0057 - mse: 0.9899 - val_loss: 0.9764 - val_accuracy: 0.0030 - val_mse: 0.9764\n",
            "Epoch 35/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9900 - accuracy: 0.0046 - mse: 0.9900 - val_loss: 0.9767 - val_accuracy: 0.0100 - val_mse: 0.9767\n",
            "Epoch 36/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9898 - accuracy: 0.0043 - mse: 0.9898 - val_loss: 0.9772 - val_accuracy: 0.0070 - val_mse: 0.9772\n",
            "Epoch 37/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9898 - accuracy: 0.0058 - mse: 0.9898 - val_loss: 0.9767 - val_accuracy: 0.0040 - val_mse: 0.9767\n",
            "Epoch 38/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9897 - accuracy: 0.0053 - mse: 0.9897 - val_loss: 0.9773 - val_accuracy: 0.0020 - val_mse: 0.9773\n",
            "Epoch 39/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9899 - accuracy: 0.0034 - mse: 0.9899 - val_loss: 0.9768 - val_accuracy: 0.0080 - val_mse: 0.9768\n",
            "Epoch 40/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9898 - accuracy: 0.0048 - mse: 0.9898 - val_loss: 0.9774 - val_accuracy: 0.0060 - val_mse: 0.9774\n",
            "Epoch 41/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9898 - accuracy: 0.0047 - mse: 0.9898 - val_loss: 0.9768 - val_accuracy: 0.0060 - val_mse: 0.9768\n",
            "Epoch 42/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9897 - accuracy: 0.0056 - mse: 0.9897 - val_loss: 0.9770 - val_accuracy: 0.0090 - val_mse: 0.9770\n",
            "Epoch 43/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9898 - accuracy: 0.0042 - mse: 0.9898 - val_loss: 0.9767 - val_accuracy: 0.0030 - val_mse: 0.9767\n",
            "Epoch 44/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9896 - accuracy: 0.0056 - mse: 0.9896 - val_loss: 0.9771 - val_accuracy: 0.0050 - val_mse: 0.9771\n",
            "Epoch 45/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9896 - accuracy: 0.0041 - mse: 0.9896 - val_loss: 0.9765 - val_accuracy: 0.0050 - val_mse: 0.9765\n",
            "Epoch 46/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9898 - accuracy: 0.0049 - mse: 0.9898 - val_loss: 0.9769 - val_accuracy: 0.0030 - val_mse: 0.9769\n",
            "Epoch 47/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9897 - accuracy: 0.0038 - mse: 0.9897 - val_loss: 0.9768 - val_accuracy: 0.0050 - val_mse: 0.9768\n",
            "Epoch 48/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9897 - accuracy: 0.0052 - mse: 0.9897 - val_loss: 0.9766 - val_accuracy: 0.0020 - val_mse: 0.9766\n",
            "Epoch 49/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9897 - accuracy: 0.0038 - mse: 0.9897 - val_loss: 0.9767 - val_accuracy: 0.0110 - val_mse: 0.9767\n",
            "Epoch 50/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9896 - accuracy: 0.0050 - mse: 0.9896 - val_loss: 0.9768 - val_accuracy: 0.0060 - val_mse: 0.9768\n",
            "Epoch 51/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9897 - accuracy: 0.0058 - mse: 0.9897 - val_loss: 0.9767 - val_accuracy: 0.0130 - val_mse: 0.9767\n",
            "Epoch 52/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9895 - accuracy: 0.0054 - mse: 0.9895 - val_loss: 0.9769 - val_accuracy: 0.0100 - val_mse: 0.9769\n",
            "Epoch 53/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9895 - accuracy: 0.0048 - mse: 0.9895 - val_loss: 0.9767 - val_accuracy: 0.0100 - val_mse: 0.9767\n",
            "Epoch 54/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9898 - accuracy: 0.0059 - mse: 0.9898 - val_loss: 0.9766 - val_accuracy: 0.0020 - val_mse: 0.9766\n",
            "Epoch 55/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9896 - accuracy: 0.0043 - mse: 0.9896 - val_loss: 0.9762 - val_accuracy: 0.0090 - val_mse: 0.9762\n",
            "Epoch 56/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9894 - accuracy: 0.0058 - mse: 0.9894 - val_loss: 0.9765 - val_accuracy: 0.0060 - val_mse: 0.9765\n",
            "Epoch 57/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9896 - accuracy: 0.0042 - mse: 0.9896 - val_loss: 0.9767 - val_accuracy: 0.0080 - val_mse: 0.9767\n",
            "Epoch 58/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0058 - mse: 0.9894 - val_loss: 0.9768 - val_accuracy: 0.0100 - val_mse: 0.9768\n",
            "Epoch 59/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9897 - accuracy: 0.0048 - mse: 0.9897 - val_loss: 0.9768 - val_accuracy: 0.0110 - val_mse: 0.9768\n",
            "Epoch 60/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9895 - accuracy: 0.0051 - mse: 0.9895 - val_loss: 0.9769 - val_accuracy: 0.0060 - val_mse: 0.9769\n",
            "Epoch 61/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0040 - mse: 0.9894 - val_loss: 0.9769 - val_accuracy: 0.0090 - val_mse: 0.9769\n",
            "Epoch 62/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9896 - accuracy: 0.0041 - mse: 0.9896 - val_loss: 0.9766 - val_accuracy: 0.0050 - val_mse: 0.9766\n",
            "Epoch 63/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0047 - mse: 0.9894 - val_loss: 0.9767 - val_accuracy: 0.0050 - val_mse: 0.9767\n",
            "Epoch 64/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9896 - accuracy: 0.0048 - mse: 0.9896 - val_loss: 0.9768 - val_accuracy: 0.0070 - val_mse: 0.9768\n",
            "Epoch 65/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9895 - accuracy: 0.0051 - mse: 0.9895 - val_loss: 0.9767 - val_accuracy: 0.0060 - val_mse: 0.9767\n",
            "Epoch 66/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9895 - accuracy: 0.0061 - mse: 0.9895 - val_loss: 0.9766 - val_accuracy: 0.0100 - val_mse: 0.9766\n",
            "Epoch 67/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9895 - accuracy: 0.0044 - mse: 0.9895 - val_loss: 0.9765 - val_accuracy: 0.0060 - val_mse: 0.9765\n",
            "Epoch 68/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0043 - mse: 0.9894 - val_loss: 0.9767 - val_accuracy: 0.0050 - val_mse: 0.9767\n",
            "Epoch 69/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9896 - accuracy: 0.0057 - mse: 0.9896 - val_loss: 0.9767 - val_accuracy: 0.0060 - val_mse: 0.9767\n",
            "Epoch 70/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9895 - accuracy: 0.0040 - mse: 0.9895 - val_loss: 0.9763 - val_accuracy: 0.0040 - val_mse: 0.9763\n",
            "Epoch 71/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0051 - mse: 0.9894 - val_loss: 0.9765 - val_accuracy: 0.0050 - val_mse: 0.9765\n",
            "Epoch 72/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9895 - accuracy: 0.0051 - mse: 0.9895 - val_loss: 0.9765 - val_accuracy: 0.0060 - val_mse: 0.9765\n",
            "Epoch 73/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9893 - accuracy: 0.0048 - mse: 0.9893 - val_loss: 0.9767 - val_accuracy: 0.0050 - val_mse: 0.9767\n",
            "Epoch 74/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0036 - mse: 0.9893 - val_loss: 0.9767 - val_accuracy: 0.0050 - val_mse: 0.9767\n",
            "Epoch 75/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0052 - mse: 0.9894 - val_loss: 0.9765 - val_accuracy: 0.0100 - val_mse: 0.9765\n",
            "Epoch 76/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0057 - mse: 0.9893 - val_loss: 0.9767 - val_accuracy: 0.0070 - val_mse: 0.9767\n",
            "Epoch 77/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9892 - accuracy: 0.0051 - mse: 0.9892 - val_loss: 0.9769 - val_accuracy: 0.0050 - val_mse: 0.9769\n",
            "Epoch 78/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0049 - mse: 0.9893 - val_loss: 0.9767 - val_accuracy: 0.0040 - val_mse: 0.9767\n",
            "Epoch 79/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0051 - mse: 0.9894 - val_loss: 0.9767 - val_accuracy: 0.0050 - val_mse: 0.9767\n",
            "Epoch 80/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0062 - mse: 0.9894 - val_loss: 0.9765 - val_accuracy: 0.0070 - val_mse: 0.9765\n",
            "Epoch 81/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0041 - mse: 0.9893 - val_loss: 0.9764 - val_accuracy: 0.0110 - val_mse: 0.9764\n",
            "Epoch 82/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0051 - mse: 0.9894 - val_loss: 0.9766 - val_accuracy: 0.0050 - val_mse: 0.9766\n",
            "Epoch 83/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0051 - mse: 0.9893 - val_loss: 0.9767 - val_accuracy: 0.0050 - val_mse: 0.9767\n",
            "Epoch 84/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0037 - mse: 0.9893 - val_loss: 0.9767 - val_accuracy: 0.0050 - val_mse: 0.9767\n",
            "Epoch 85/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0053 - mse: 0.9893 - val_loss: 0.9763 - val_accuracy: 0.0080 - val_mse: 0.9763\n",
            "Epoch 86/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0060 - mse: 0.9893 - val_loss: 0.9763 - val_accuracy: 0.0020 - val_mse: 0.9763\n",
            "Epoch 87/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0052 - mse: 0.9893 - val_loss: 0.9765 - val_accuracy: 0.0070 - val_mse: 0.9765\n",
            "Epoch 88/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9893 - accuracy: 0.0041 - mse: 0.9893 - val_loss: 0.9768 - val_accuracy: 0.0040 - val_mse: 0.9768\n",
            "Epoch 89/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0048 - mse: 0.9894 - val_loss: 0.9766 - val_accuracy: 0.0070 - val_mse: 0.9766\n",
            "Epoch 90/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9892 - accuracy: 0.0048 - mse: 0.9892 - val_loss: 0.9768 - val_accuracy: 0.0040 - val_mse: 0.9768\n",
            "Epoch 91/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0039 - mse: 0.9893 - val_loss: 0.9768 - val_accuracy: 0.0060 - val_mse: 0.9768\n",
            "Epoch 92/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0053 - mse: 0.9894 - val_loss: 0.9766 - val_accuracy: 0.0080 - val_mse: 0.9766\n",
            "Epoch 93/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0060 - mse: 0.9893 - val_loss: 0.9764 - val_accuracy: 0.0040 - val_mse: 0.9764\n",
            "Epoch 94/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9891 - accuracy: 0.0050 - mse: 0.9891 - val_loss: 0.9763 - val_accuracy: 0.0090 - val_mse: 0.9763\n",
            "Epoch 95/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0058 - mse: 0.9893 - val_loss: 0.9768 - val_accuracy: 0.0050 - val_mse: 0.9768\n",
            "Epoch 96/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9892 - accuracy: 0.0060 - mse: 0.9892 - val_loss: 0.9766 - val_accuracy: 0.0060 - val_mse: 0.9766\n",
            "Epoch 97/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9892 - accuracy: 0.0041 - mse: 0.9892 - val_loss: 0.9765 - val_accuracy: 0.0050 - val_mse: 0.9765\n",
            "Epoch 98/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9892 - accuracy: 0.0064 - mse: 0.9892 - val_loss: 0.9766 - val_accuracy: 0.0050 - val_mse: 0.9766\n",
            "Epoch 99/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9892 - accuracy: 0.0057 - mse: 0.9892 - val_loss: 0.9766 - val_accuracy: 0.0080 - val_mse: 0.9766\n",
            "Epoch 100/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9891 - accuracy: 0.0038 - mse: 0.9891 - val_loss: 0.9764 - val_accuracy: 0.0040 - val_mse: 0.9764\n",
            "Epoch 101/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9894 - accuracy: 0.0057 - mse: 0.9894 - val_loss: 0.9764 - val_accuracy: 0.0050 - val_mse: 0.9764\n",
            "Epoch 102/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9893 - accuracy: 0.0048 - mse: 0.9893 - val_loss: 0.9764 - val_accuracy: 0.0100 - val_mse: 0.9764\n",
            "Epoch 103/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0063 - mse: 0.9890 - val_loss: 0.9765 - val_accuracy: 0.0060 - val_mse: 0.9765\n",
            "Epoch 104/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9891 - accuracy: 0.0058 - mse: 0.9891 - val_loss: 0.9766 - val_accuracy: 0.0050 - val_mse: 0.9766\n",
            "Epoch 105/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0051 - mse: 0.9893 - val_loss: 0.9765 - val_accuracy: 0.0120 - val_mse: 0.9765\n",
            "Epoch 106/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0047 - mse: 0.9893 - val_loss: 0.9766 - val_accuracy: 0.0050 - val_mse: 0.9766\n",
            "Epoch 107/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9892 - accuracy: 0.0051 - mse: 0.9892 - val_loss: 0.9762 - val_accuracy: 0.0050 - val_mse: 0.9762\n",
            "Epoch 108/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9893 - accuracy: 0.0061 - mse: 0.9893 - val_loss: 0.9766 - val_accuracy: 0.0040 - val_mse: 0.9766\n",
            "Epoch 109/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9892 - accuracy: 0.0061 - mse: 0.9892 - val_loss: 0.9765 - val_accuracy: 0.0020 - val_mse: 0.9765\n",
            "Epoch 110/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0057 - mse: 0.9890 - val_loss: 0.9765 - val_accuracy: 0.0070 - val_mse: 0.9765\n",
            "Epoch 111/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9891 - accuracy: 0.0054 - mse: 0.9891 - val_loss: 0.9765 - val_accuracy: 0.0080 - val_mse: 0.9765\n",
            "Epoch 112/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0051 - mse: 0.9890 - val_loss: 0.9762 - val_accuracy: 0.0050 - val_mse: 0.9762\n",
            "Epoch 113/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0041 - mse: 0.9890 - val_loss: 0.9770 - val_accuracy: 0.0070 - val_mse: 0.9770\n",
            "Epoch 114/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9891 - accuracy: 0.0053 - mse: 0.9891 - val_loss: 0.9769 - val_accuracy: 0.0060 - val_mse: 0.9769\n",
            "Epoch 115/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0063 - mse: 0.9889 - val_loss: 0.9767 - val_accuracy: 0.0060 - val_mse: 0.9767\n",
            "Epoch 116/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9891 - accuracy: 0.0047 - mse: 0.9891 - val_loss: 0.9756 - val_accuracy: 0.0050 - val_mse: 0.9756\n",
            "Epoch 117/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0040 - mse: 0.9889 - val_loss: 0.9764 - val_accuracy: 0.0130 - val_mse: 0.9764\n",
            "Epoch 118/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9891 - accuracy: 0.0038 - mse: 0.9891 - val_loss: 0.9765 - val_accuracy: 0.0100 - val_mse: 0.9765\n",
            "Epoch 119/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9891 - accuracy: 0.0052 - mse: 0.9891 - val_loss: 0.9764 - val_accuracy: 0.0060 - val_mse: 0.9764\n",
            "Epoch 120/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0051 - mse: 0.9889 - val_loss: 0.9765 - val_accuracy: 0.0060 - val_mse: 0.9765\n",
            "Epoch 121/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0043 - mse: 0.9889 - val_loss: 0.9766 - val_accuracy: 0.0070 - val_mse: 0.9766\n",
            "Epoch 122/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0051 - mse: 0.9890 - val_loss: 0.9767 - val_accuracy: 0.0020 - val_mse: 0.9767\n",
            "Epoch 123/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0061 - mse: 0.9890 - val_loss: 0.9764 - val_accuracy: 0.0130 - val_mse: 0.9764\n",
            "Epoch 124/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9891 - accuracy: 0.0054 - mse: 0.9891 - val_loss: 0.9764 - val_accuracy: 0.0050 - val_mse: 0.9764\n",
            "Epoch 125/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0046 - mse: 0.9890 - val_loss: 0.9763 - val_accuracy: 0.0050 - val_mse: 0.9763\n",
            "Epoch 126/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0051 - mse: 0.9890 - val_loss: 0.9762 - val_accuracy: 0.0050 - val_mse: 0.9762\n",
            "Epoch 127/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0058 - mse: 0.9889 - val_loss: 0.9763 - val_accuracy: 0.0020 - val_mse: 0.9763\n",
            "Epoch 128/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0054 - mse: 0.9890 - val_loss: 0.9764 - val_accuracy: 0.0050 - val_mse: 0.9764\n",
            "Epoch 129/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0043 - mse: 0.9889 - val_loss: 0.9765 - val_accuracy: 0.0040 - val_mse: 0.9765\n",
            "Epoch 130/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0051 - mse: 0.9889 - val_loss: 0.9764 - val_accuracy: 0.0070 - val_mse: 0.9764\n",
            "Epoch 131/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0052 - mse: 0.9889 - val_loss: 0.9761 - val_accuracy: 0.0040 - val_mse: 0.9761\n",
            "Epoch 132/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9888 - accuracy: 0.0047 - mse: 0.9888 - val_loss: 0.9766 - val_accuracy: 0.0070 - val_mse: 0.9766\n",
            "Epoch 133/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0058 - mse: 0.9890 - val_loss: 0.9763 - val_accuracy: 0.0070 - val_mse: 0.9763\n",
            "Epoch 134/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0053 - mse: 0.9890 - val_loss: 0.9765 - val_accuracy: 0.0070 - val_mse: 0.9765\n",
            "Epoch 135/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9888 - accuracy: 0.0044 - mse: 0.9888 - val_loss: 0.9765 - val_accuracy: 0.0110 - val_mse: 0.9765\n",
            "Epoch 136/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0044 - mse: 0.9889 - val_loss: 0.9763 - val_accuracy: 0.0040 - val_mse: 0.9763\n",
            "Epoch 137/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0057 - mse: 0.9889 - val_loss: 0.9764 - val_accuracy: 0.0060 - val_mse: 0.9764\n",
            "Epoch 138/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9888 - accuracy: 0.0052 - mse: 0.9888 - val_loss: 0.9764 - val_accuracy: 0.0040 - val_mse: 0.9764\n",
            "Epoch 139/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0047 - mse: 0.9889 - val_loss: 0.9765 - val_accuracy: 0.0080 - val_mse: 0.9765\n",
            "Epoch 140/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0042 - mse: 0.9889 - val_loss: 0.9769 - val_accuracy: 0.0050 - val_mse: 0.9769\n",
            "Epoch 141/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9888 - accuracy: 0.0057 - mse: 0.9888 - val_loss: 0.9762 - val_accuracy: 0.0070 - val_mse: 0.9762\n",
            "Epoch 142/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0060 - mse: 0.9889 - val_loss: 0.9765 - val_accuracy: 0.0050 - val_mse: 0.9765\n",
            "Epoch 143/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9890 - accuracy: 0.0050 - mse: 0.9890 - val_loss: 0.9762 - val_accuracy: 0.0080 - val_mse: 0.9762\n",
            "Epoch 144/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0049 - mse: 0.9887 - val_loss: 0.9765 - val_accuracy: 0.0060 - val_mse: 0.9765\n",
            "Epoch 145/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9888 - accuracy: 0.0048 - mse: 0.9888 - val_loss: 0.9762 - val_accuracy: 0.0060 - val_mse: 0.9762\n",
            "Epoch 146/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0044 - mse: 0.9889 - val_loss: 0.9764 - val_accuracy: 0.0020 - val_mse: 0.9764\n",
            "Epoch 147/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0069 - mse: 0.9889 - val_loss: 0.9761 - val_accuracy: 0.0070 - val_mse: 0.9761\n",
            "Epoch 148/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0042 - mse: 0.9889 - val_loss: 0.9762 - val_accuracy: 0.0090 - val_mse: 0.9762\n",
            "Epoch 149/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9888 - accuracy: 0.0037 - mse: 0.9888 - val_loss: 0.9764 - val_accuracy: 0.0050 - val_mse: 0.9764\n",
            "Epoch 150/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0058 - mse: 0.9887 - val_loss: 0.9761 - val_accuracy: 0.0090 - val_mse: 0.9761\n",
            "Epoch 151/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0050 - mse: 0.9889 - val_loss: 0.9765 - val_accuracy: 0.0040 - val_mse: 0.9765\n",
            "Epoch 152/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0053 - mse: 0.9887 - val_loss: 0.9767 - val_accuracy: 0.0070 - val_mse: 0.9767\n",
            "Epoch 153/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0050 - mse: 0.9887 - val_loss: 0.9763 - val_accuracy: 0.0050 - val_mse: 0.9763\n",
            "Epoch 154/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0052 - mse: 0.9887 - val_loss: 0.9766 - val_accuracy: 0.0070 - val_mse: 0.9766\n",
            "Epoch 155/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0051 - mse: 0.9889 - val_loss: 0.9762 - val_accuracy: 0.0060 - val_mse: 0.9762\n",
            "Epoch 156/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9888 - accuracy: 0.0046 - mse: 0.9888 - val_loss: 0.9765 - val_accuracy: 0.0070 - val_mse: 0.9765\n",
            "Epoch 157/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0047 - mse: 0.9887 - val_loss: 0.9764 - val_accuracy: 0.0040 - val_mse: 0.9764\n",
            "Epoch 158/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0059 - mse: 0.9889 - val_loss: 0.9764 - val_accuracy: 0.0060 - val_mse: 0.9764\n",
            "Epoch 159/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0043 - mse: 0.9887 - val_loss: 0.9760 - val_accuracy: 0.0030 - val_mse: 0.9760\n",
            "Epoch 160/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9886 - accuracy: 0.0060 - mse: 0.9886 - val_loss: 0.9761 - val_accuracy: 0.0060 - val_mse: 0.9761\n",
            "Epoch 161/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0066 - mse: 0.9887 - val_loss: 0.9763 - val_accuracy: 0.0040 - val_mse: 0.9763\n",
            "Epoch 162/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0044 - mse: 0.9887 - val_loss: 0.9762 - val_accuracy: 0.0050 - val_mse: 0.9762\n",
            "Epoch 163/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9888 - accuracy: 0.0063 - mse: 0.9888 - val_loss: 0.9763 - val_accuracy: 0.0050 - val_mse: 0.9763\n",
            "Epoch 164/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9887 - accuracy: 0.0044 - mse: 0.9887 - val_loss: 0.9759 - val_accuracy: 0.0050 - val_mse: 0.9759\n",
            "Epoch 165/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9886 - accuracy: 0.0070 - mse: 0.9886 - val_loss: 0.9762 - val_accuracy: 0.0090 - val_mse: 0.9762\n",
            "Epoch 166/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0057 - mse: 0.9887 - val_loss: 0.9760 - val_accuracy: 0.0060 - val_mse: 0.9760\n",
            "Epoch 167/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0058 - mse: 0.9887 - val_loss: 0.9762 - val_accuracy: 0.0070 - val_mse: 0.9762\n",
            "Epoch 168/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0051 - mse: 0.9887 - val_loss: 0.9763 - val_accuracy: 0.0060 - val_mse: 0.9763\n",
            "Epoch 169/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9888 - accuracy: 0.0048 - mse: 0.9888 - val_loss: 0.9760 - val_accuracy: 0.0060 - val_mse: 0.9760\n",
            "Epoch 170/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0042 - mse: 0.9887 - val_loss: 0.9764 - val_accuracy: 0.0080 - val_mse: 0.9764\n",
            "Epoch 171/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9889 - accuracy: 0.0056 - mse: 0.9889 - val_loss: 0.9766 - val_accuracy: 0.0070 - val_mse: 0.9766\n",
            "Epoch 172/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0040 - mse: 0.9887 - val_loss: 0.9761 - val_accuracy: 0.0030 - val_mse: 0.9761\n",
            "Epoch 173/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0042 - mse: 0.9887 - val_loss: 0.9763 - val_accuracy: 0.0050 - val_mse: 0.9763\n",
            "Epoch 174/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9886 - accuracy: 0.0053 - mse: 0.9886 - val_loss: 0.9763 - val_accuracy: 0.0060 - val_mse: 0.9763\n",
            "Epoch 175/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0056 - mse: 0.9887 - val_loss: 0.9766 - val_accuracy: 0.0070 - val_mse: 0.9766\n",
            "Epoch 176/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9887 - accuracy: 0.0044 - mse: 0.9887 - val_loss: 0.9759 - val_accuracy: 0.0080 - val_mse: 0.9759\n",
            "Epoch 177/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9885 - accuracy: 0.0044 - mse: 0.9885 - val_loss: 0.9757 - val_accuracy: 0.0080 - val_mse: 0.9757\n",
            "Epoch 178/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9885 - accuracy: 0.0050 - mse: 0.9885 - val_loss: 0.9761 - val_accuracy: 0.0090 - val_mse: 0.9761\n",
            "Epoch 179/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0051 - mse: 0.9887 - val_loss: 0.9759 - val_accuracy: 0.0030 - val_mse: 0.9759\n",
            "Epoch 180/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9886 - accuracy: 0.0058 - mse: 0.9886 - val_loss: 0.9760 - val_accuracy: 0.0050 - val_mse: 0.9760\n",
            "Epoch 181/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9887 - accuracy: 0.0050 - mse: 0.9887 - val_loss: 0.9763 - val_accuracy: 0.0100 - val_mse: 0.9763\n",
            "Epoch 182/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9886 - accuracy: 0.0058 - mse: 0.9886 - val_loss: 0.9762 - val_accuracy: 0.0070 - val_mse: 0.9762\n",
            "Epoch 183/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9885 - accuracy: 0.0050 - mse: 0.9885 - val_loss: 0.9763 - val_accuracy: 0.0060 - val_mse: 0.9763\n",
            "Epoch 184/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9885 - accuracy: 0.0053 - mse: 0.9885 - val_loss: 0.9760 - val_accuracy: 0.0040 - val_mse: 0.9760\n",
            "Epoch 185/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9886 - accuracy: 0.0054 - mse: 0.9886 - val_loss: 0.9762 - val_accuracy: 0.0060 - val_mse: 0.9762\n",
            "Epoch 186/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9886 - accuracy: 0.0048 - mse: 0.9886 - val_loss: 0.9764 - val_accuracy: 0.0070 - val_mse: 0.9764\n",
            "Epoch 187/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9885 - accuracy: 0.0051 - mse: 0.9885 - val_loss: 0.9758 - val_accuracy: 0.0060 - val_mse: 0.9758\n",
            "Epoch 188/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9885 - accuracy: 0.0056 - mse: 0.9885 - val_loss: 0.9762 - val_accuracy: 0.0080 - val_mse: 0.9762\n",
            "Epoch 189/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9885 - accuracy: 0.0059 - mse: 0.9885 - val_loss: 0.9761 - val_accuracy: 0.0050 - val_mse: 0.9761\n",
            "Epoch 190/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9885 - accuracy: 0.0050 - mse: 0.9885 - val_loss: 0.9765 - val_accuracy: 0.0090 - val_mse: 0.9765\n",
            "Epoch 191/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0049 - mse: 0.9884 - val_loss: 0.9760 - val_accuracy: 0.0040 - val_mse: 0.9760\n",
            "Epoch 192/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9885 - accuracy: 0.0051 - mse: 0.9885 - val_loss: 0.9766 - val_accuracy: 0.0080 - val_mse: 0.9766\n",
            "Epoch 193/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0058 - mse: 0.9884 - val_loss: 0.9763 - val_accuracy: 0.0070 - val_mse: 0.9763\n",
            "Epoch 194/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0040 - mse: 0.9884 - val_loss: 0.9762 - val_accuracy: 0.0060 - val_mse: 0.9762\n",
            "Epoch 195/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9888 - accuracy: 0.0060 - mse: 0.9888 - val_loss: 0.9761 - val_accuracy: 0.0060 - val_mse: 0.9761\n",
            "Epoch 196/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0049 - mse: 0.9884 - val_loss: 0.9761 - val_accuracy: 0.0050 - val_mse: 0.9761\n",
            "Epoch 197/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9885 - accuracy: 0.0047 - mse: 0.9885 - val_loss: 0.9763 - val_accuracy: 0.0100 - val_mse: 0.9763\n",
            "Epoch 198/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0050 - mse: 0.9884 - val_loss: 0.9760 - val_accuracy: 0.0020 - val_mse: 0.9760\n",
            "Epoch 199/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0041 - mse: 0.9884 - val_loss: 0.9762 - val_accuracy: 0.0070 - val_mse: 0.9762\n",
            "Epoch 200/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0052 - mse: 0.9884 - val_loss: 0.9763 - val_accuracy: 0.0080 - val_mse: 0.9763\n",
            "Epoch 201/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0058 - mse: 0.9884 - val_loss: 0.9763 - val_accuracy: 0.0060 - val_mse: 0.9763\n",
            "Epoch 202/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0061 - mse: 0.9884 - val_loss: 0.9764 - val_accuracy: 0.0080 - val_mse: 0.9764\n",
            "Epoch 203/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0052 - mse: 0.9884 - val_loss: 0.9762 - val_accuracy: 0.0060 - val_mse: 0.9762\n",
            "Epoch 204/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0068 - mse: 0.9884 - val_loss: 0.9760 - val_accuracy: 0.0080 - val_mse: 0.9760\n",
            "Epoch 205/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0048 - mse: 0.9884 - val_loss: 0.9763 - val_accuracy: 0.0050 - val_mse: 0.9763\n",
            "Epoch 206/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0049 - mse: 0.9882 - val_loss: 0.9760 - val_accuracy: 0.0070 - val_mse: 0.9760\n",
            "Epoch 207/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0039 - mse: 0.9884 - val_loss: 0.9760 - val_accuracy: 0.0090 - val_mse: 0.9760\n",
            "Epoch 208/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0054 - mse: 0.9884 - val_loss: 0.9766 - val_accuracy: 0.0040 - val_mse: 0.9766\n",
            "Epoch 209/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0040 - mse: 0.9882 - val_loss: 0.9763 - val_accuracy: 0.0060 - val_mse: 0.9763\n",
            "Epoch 210/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0059 - mse: 0.9884 - val_loss: 0.9764 - val_accuracy: 0.0060 - val_mse: 0.9764\n",
            "Epoch 211/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0059 - mse: 0.9884 - val_loss: 0.9757 - val_accuracy: 0.0050 - val_mse: 0.9757\n",
            "Epoch 212/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0052 - mse: 0.9882 - val_loss: 0.9760 - val_accuracy: 0.0080 - val_mse: 0.9760\n",
            "Epoch 213/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0053 - mse: 0.9884 - val_loss: 0.9763 - val_accuracy: 0.0040 - val_mse: 0.9763\n",
            "Epoch 214/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0054 - mse: 0.9884 - val_loss: 0.9758 - val_accuracy: 0.0080 - val_mse: 0.9758\n",
            "Epoch 215/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9883 - accuracy: 0.0052 - mse: 0.9883 - val_loss: 0.9762 - val_accuracy: 0.0100 - val_mse: 0.9762\n",
            "Epoch 216/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0057 - mse: 0.9884 - val_loss: 0.9761 - val_accuracy: 0.0090 - val_mse: 0.9761\n",
            "Epoch 217/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0037 - mse: 0.9884 - val_loss: 0.9760 - val_accuracy: 0.0090 - val_mse: 0.9760\n",
            "Epoch 218/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9883 - accuracy: 0.0061 - mse: 0.9883 - val_loss: 0.9754 - val_accuracy: 0.0040 - val_mse: 0.9754\n",
            "Epoch 219/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9883 - accuracy: 0.0049 - mse: 0.9883 - val_loss: 0.9763 - val_accuracy: 0.0060 - val_mse: 0.9763\n",
            "Epoch 220/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0042 - mse: 0.9884 - val_loss: 0.9761 - val_accuracy: 0.0050 - val_mse: 0.9761\n",
            "Epoch 221/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0041 - mse: 0.9884 - val_loss: 0.9761 - val_accuracy: 0.0050 - val_mse: 0.9761\n",
            "Epoch 222/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9883 - accuracy: 0.0067 - mse: 0.9883 - val_loss: 0.9762 - val_accuracy: 0.0030 - val_mse: 0.9762\n",
            "Epoch 223/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9883 - accuracy: 0.0047 - mse: 0.9883 - val_loss: 0.9761 - val_accuracy: 0.0040 - val_mse: 0.9761\n",
            "Epoch 224/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0040 - mse: 0.9884 - val_loss: 0.9759 - val_accuracy: 0.0070 - val_mse: 0.9759\n",
            "Epoch 225/250\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 0.9882 - accuracy: 0.0058 - mse: 0.9882 - val_loss: 0.9762 - val_accuracy: 0.0040 - val_mse: 0.9762\n",
            "Epoch 226/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0067 - mse: 0.9882 - val_loss: 0.9760 - val_accuracy: 0.0030 - val_mse: 0.9760\n",
            "Epoch 227/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0052 - mse: 0.9882 - val_loss: 0.9763 - val_accuracy: 0.0070 - val_mse: 0.9763\n",
            "Epoch 228/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9883 - accuracy: 0.0049 - mse: 0.9883 - val_loss: 0.9757 - val_accuracy: 0.0110 - val_mse: 0.9757\n",
            "Epoch 229/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0053 - mse: 0.9882 - val_loss: 0.9762 - val_accuracy: 0.0110 - val_mse: 0.9762\n",
            "Epoch 230/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9883 - accuracy: 0.0058 - mse: 0.9883 - val_loss: 0.9761 - val_accuracy: 0.0060 - val_mse: 0.9761\n",
            "Epoch 231/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9883 - accuracy: 0.0054 - mse: 0.9883 - val_loss: 0.9762 - val_accuracy: 0.0030 - val_mse: 0.9762\n",
            "Epoch 232/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9884 - accuracy: 0.0054 - mse: 0.9884 - val_loss: 0.9757 - val_accuracy: 0.0040 - val_mse: 0.9757\n",
            "Epoch 233/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0058 - mse: 0.9882 - val_loss: 0.9762 - val_accuracy: 0.0040 - val_mse: 0.9762\n",
            "Epoch 234/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9883 - accuracy: 0.0052 - mse: 0.9883 - val_loss: 0.9757 - val_accuracy: 0.0090 - val_mse: 0.9757\n",
            "Epoch 235/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9883 - accuracy: 0.0044 - mse: 0.9883 - val_loss: 0.9763 - val_accuracy: 0.0050 - val_mse: 0.9763\n",
            "Epoch 236/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9883 - accuracy: 0.0053 - mse: 0.9883 - val_loss: 0.9763 - val_accuracy: 0.0040 - val_mse: 0.9763\n",
            "Epoch 237/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0058 - mse: 0.9882 - val_loss: 0.9759 - val_accuracy: 0.0050 - val_mse: 0.9759\n",
            "Epoch 238/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0058 - mse: 0.9882 - val_loss: 0.9758 - val_accuracy: 0.0050 - val_mse: 0.9758\n",
            "Epoch 239/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0052 - mse: 0.9882 - val_loss: 0.9763 - val_accuracy: 0.0040 - val_mse: 0.9763\n",
            "Epoch 240/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0051 - mse: 0.9882 - val_loss: 0.9760 - val_accuracy: 0.0040 - val_mse: 0.9760\n",
            "Epoch 241/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9880 - accuracy: 0.0050 - mse: 0.9880 - val_loss: 0.9757 - val_accuracy: 0.0030 - val_mse: 0.9757\n",
            "Epoch 242/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0051 - mse: 0.9882 - val_loss: 0.9760 - val_accuracy: 0.0040 - val_mse: 0.9760\n",
            "Epoch 243/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9880 - accuracy: 0.0059 - mse: 0.9880 - val_loss: 0.9761 - val_accuracy: 0.0100 - val_mse: 0.9761\n",
            "Epoch 244/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9880 - accuracy: 0.0044 - mse: 0.9880 - val_loss: 0.9758 - val_accuracy: 0.0090 - val_mse: 0.9758\n",
            "Epoch 245/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0050 - mse: 0.9882 - val_loss: 0.9760 - val_accuracy: 0.0060 - val_mse: 0.9760\n",
            "Epoch 246/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9881 - accuracy: 0.0058 - mse: 0.9881 - val_loss: 0.9761 - val_accuracy: 0.0050 - val_mse: 0.9761\n",
            "Epoch 247/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9885 - accuracy: 0.0054 - mse: 0.9885 - val_loss: 0.9760 - val_accuracy: 0.0060 - val_mse: 0.9760\n",
            "Epoch 248/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0056 - mse: 0.9882 - val_loss: 0.9761 - val_accuracy: 0.0060 - val_mse: 0.9761\n",
            "Epoch 249/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9881 - accuracy: 0.0038 - mse: 0.9881 - val_loss: 0.9760 - val_accuracy: 0.0050 - val_mse: 0.9760\n",
            "Epoch 250/250\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 0.9882 - accuracy: 0.0054 - mse: 0.9882 - val_loss: 0.9761 - val_accuracy: 0.0060 - val_mse: 0.9761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVpSwHw3GtYh"
      },
      "source": [
        "class Eavesdropper():\n",
        "\n",
        "  def __init__(self):\n",
        "    spoofed_data = None\n",
        "\n",
        "  def get_spoofing_data(self,received_data):\n",
        "    \n",
        "    mu = received_data.mean(axis=0)\n",
        "    sigma = received_data.std(axis=0)\n",
        "    s = np.random.normal(mu, sigma, 16)\n",
        "    # print('s = {}'.format(s))\n",
        "\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OshgbwYeM6HY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "40f3e1fa-6091-4104-dbd9-25c7579d9039"
      },
      "source": [
        "data= comSys.data\n",
        "# print(data[55,:])\n",
        "receivedSignals = comSys.receivedSignals\n",
        "received_data = receivedSignals.predict(data)\n",
        "print(comSys.transmiter.predict(data))\n",
        "print(received_data[556,:])\n",
        "print((received_data.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.44295436 -1.6668426   0.06948718 ... -0.43950814  0.89805067\n",
            "   1.5630552 ]\n",
            " [-0.9133768  -2.3439064   0.02932254 ...  1.4176849  -0.07746894\n",
            "  -0.6159278 ]\n",
            " [-1.1513939   0.89288217 -0.65031326 ... -0.8763612  -0.8328495\n",
            "   0.27435213]\n",
            " ...\n",
            " [-0.6104111  -0.97246736 -1.6762632  ...  0.44092155  0.6123854\n",
            "  -2.0561936 ]\n",
            " [ 0.6054493   0.86190313  0.73852754 ... -0.69049454 -0.3204767\n",
            "  -2.5831428 ]\n",
            " [ 0.50767046  0.5702033  -1.0942677  ... -0.14292426 -0.54425704\n",
            "  -1.8921889 ]]\n",
            "[-0.26839408  1.2769638  -1.6012094   1.1510174  -1.1552008   1.1718132\n",
            " -0.72767913 -2.0026093   0.18543014  1.1828359   0.45611706 -0.5925846\n",
            " -0.3868359  -0.6131649  -0.589421    0.54342055]\n",
            "(10000, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AbCCKRnNAc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "7110fef8-83af-4974-9587-2e3780210384"
      },
      "source": [
        "eve = Eavesdropper()\n",
        "eve_data = eve.get_spoofing_data(received_data)\n",
        "print(eve_data)\n",
        "# As this data should pass channel to reach receiver a WGN must be added to signal not regarding interference\n",
        "\n",
        "# Generating White Noise:\n",
        "mean = 0\n",
        "std = 1 \n",
        "num_samples = 16\n",
        "noise_samples = np.random.normal(mean, std, size=num_samples)\n",
        "print(noise_samples)\n",
        "\n",
        "for i in range(len(eve_data)):\n",
        "  eve_data[i] = eve_data[i] + noise_samples[i]\n",
        "\n",
        "print(eve_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.81193017  1.20901747 -0.507154   -0.70800489 -0.39598999  0.7327616\n",
            " -2.30325184  0.81047995 -1.89344631 -0.20679305 -0.63575423  0.26202554\n",
            " -0.85996762  0.64175942 -0.46995864  0.218262  ]\n",
            "[-0.58522316 -0.50402375 -0.28377977 -0.66498657 -0.67844961 -1.0781595\n",
            "  0.6696956  -0.22101152 -0.391561    0.42892016 -0.39227461  0.54836213\n",
            " -0.71610347 -1.41810467 -0.06529516  1.78384878]\n",
            "[ 0.22670701  0.70499372 -0.79093377 -1.37299146 -1.0744396  -0.3453979\n",
            " -1.63355624  0.58946843 -2.2850073   0.22212711 -1.02802884  0.81038767\n",
            " -1.57607108 -0.77634526 -0.5352538   2.00211078]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "end4dD5xNE8t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c800589a-2f99-49fa-dd51-5d70dff07a36"
      },
      "source": [
        "signal_size = 512\n",
        "dataset = np.zeros((4001,signal_size))\n",
        "print(dataset.shape)\n",
        "labels = np.zeros((4001,1))\n",
        "L,i = 4000,0\n",
        "s = np.random.uniform(0,1,L)\n",
        "\n",
        "for sample in s:\n",
        "\n",
        "  if sample>=0.5:\n",
        "    dataset[i,:] = comSys.receiver.predict(np.reshape(received_data[i,:],(1,16)))\n",
        "    labels[i,0] = 1\n",
        "  else:\n",
        "    eve_data = eve.get_spoofing_data(received_data)\n",
        "    noise_samples = np.random.normal(mean, std, size=num_samples)\n",
        "    for j in range(len(eve_data)):\n",
        "      eve_data[j] = eve_data[j] + noise_samples[j]\n",
        "\n",
        "    dataset[i,:] = comSys.receiver.predict(np.reshape(eve_data[:],(1,16)))\n",
        "    labels[i,0] = 0\n",
        "\n",
        "  i = i+1\n",
        "print(dataset[5,:])\n",
        "  # print(dataset)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4001, 512)\n",
            "[-7.69105256e-02 -5.08864969e-02  2.46835083e-01  8.10312331e-02\n",
            " -3.94043237e-01 -3.20730120e-01 -7.15623721e-02  6.87998161e-03\n",
            " -7.26418942e-02 -1.66558087e-01  4.63061407e-03 -2.29279757e-01\n",
            "  2.19899356e-01  2.84064323e-01 -2.12374806e-01  6.06866106e-02\n",
            "  2.30016977e-01  2.12132066e-01 -2.05137208e-02 -2.91353744e-02\n",
            " -1.64268762e-01 -2.12599307e-01 -1.59885883e-01  3.95349301e-02\n",
            "  9.90305990e-02 -2.49565884e-01  2.81664699e-01 -1.62282065e-01\n",
            "  4.08792607e-02  2.88706541e-01  3.25853288e-01  3.01913828e-01\n",
            "  1.53519496e-01 -8.74794424e-02 -3.92404914e-01  4.23633028e-04\n",
            "  5.57284355e-02 -4.67140935e-02  3.96417379e-02  2.35227533e-02\n",
            "  4.42630500e-01  3.64310265e-01 -6.72172159e-02 -2.62306314e-02\n",
            " -2.33315095e-01  2.01949980e-02 -1.86209962e-01 -3.45088392e-02\n",
            " -3.56389701e-01 -3.53953838e-02 -9.27515626e-02 -2.19251618e-01\n",
            " -2.67712139e-02  1.58852577e-01 -3.16673696e-01  5.56685105e-02\n",
            "  1.06975868e-01  1.76839262e-01 -2.25049585e-01  1.18159458e-01\n",
            " -1.43937245e-01  1.94877520e-01 -1.71689354e-02  3.45735326e-02\n",
            " -2.19018519e-01  2.45411173e-02 -1.11795738e-01  1.95850804e-01\n",
            "  6.96762577e-02 -7.37975165e-02  1.28242671e-01  1.02534086e-01\n",
            "  5.98318642e-03 -8.20719302e-02 -7.16982409e-03 -3.55466641e-02\n",
            "  1.33846253e-01 -3.16193193e-01 -9.04597044e-02 -2.10717246e-02\n",
            "  3.27318609e-01 -2.75701672e-01  2.63302863e-01  4.39107329e-01\n",
            "  2.66087241e-02 -3.75985622e-01 -3.56674939e-01  2.26237640e-01\n",
            " -1.87316790e-01 -2.11289555e-01 -6.59411848e-02  5.26933447e-02\n",
            " -1.86236903e-01  2.11774737e-01  2.79093057e-01 -1.29311502e-01\n",
            "  4.72399890e-02  1.76278397e-01 -8.76982659e-02 -1.54336065e-01\n",
            " -2.92357296e-01  1.86966434e-01 -1.66972473e-01  4.42797020e-02\n",
            " -8.54634941e-02 -2.54727691e-01 -2.38717243e-01  9.80692282e-02\n",
            "  2.33723685e-01  9.88487750e-02 -4.85585853e-02  5.12294531e-01\n",
            "  2.36732997e-02 -4.43746150e-02  4.44596931e-02  1.13792539e-01\n",
            "  9.33888461e-03  2.20615178e-01  6.59682676e-02  2.43427008e-01\n",
            "  1.93024248e-01  1.81375384e-01 -7.60748088e-02 -1.16392791e-01\n",
            " -2.64110327e-01  2.03774616e-01 -1.56219527e-01  2.94326723e-01\n",
            " -1.76116645e-01  1.51870281e-01 -1.48655131e-01 -2.31027510e-02\n",
            " -9.89979878e-02 -3.87157612e-02  2.71138102e-01  3.15373838e-02\n",
            "  4.79612499e-02 -6.57889619e-02 -1.88299879e-01  1.30728753e-02\n",
            "  9.53979865e-02 -7.03313202e-02 -1.70695513e-01 -2.61364549e-01\n",
            " -1.78026274e-01  3.68483394e-01 -8.56069699e-02  2.98180785e-02\n",
            "  8.30355510e-02 -9.30718184e-02 -1.64191782e-01  1.62205786e-01\n",
            " -1.55360531e-03 -1.26984313e-01  1.01921104e-01  1.84451286e-02\n",
            " -2.80945361e-01 -5.69927767e-02  7.20808133e-02  8.67198706e-02\n",
            " -2.68613130e-01 -1.24162421e-01 -2.88425773e-01 -1.31614804e-01\n",
            " -7.88138509e-02 -3.40628415e-01 -2.40212962e-01 -6.90329522e-02\n",
            " -1.48355784e-02  3.46735686e-01 -6.46598265e-02 -2.04712719e-01\n",
            " -4.10579070e-02  7.52105340e-02  7.51656517e-02 -3.30767602e-01\n",
            "  3.53272855e-01  3.66573095e-01 -1.90869108e-01  2.70873457e-01\n",
            "  1.00921899e-01 -2.68900335e-01  9.97402966e-02  2.90412977e-02\n",
            " -2.46458098e-01  3.98412831e-02 -1.98338717e-01 -3.07217222e-02\n",
            " -4.05417502e-01  9.35564339e-02 -1.93143815e-01  9.59570706e-02\n",
            " -8.96937624e-02  3.04214716e-01  4.12589014e-02  8.54782164e-02\n",
            "  7.23169520e-02 -1.29404992e-01 -9.80000347e-02  1.25827178e-01\n",
            "  5.48717566e-03 -2.96607912e-01 -1.34682968e-01  4.05715585e-01\n",
            "  1.53812721e-01 -1.20855540e-01  2.50820696e-01 -6.78396076e-02\n",
            "  9.14409608e-02  1.24426737e-01 -9.75941122e-02 -2.81678110e-01\n",
            "  9.05471966e-02 -2.16001272e-01 -1.35665126e-02  1.47883017e-02\n",
            " -2.17561021e-01 -2.47622039e-02  2.15581253e-01  2.15504006e-01\n",
            " -3.25340517e-02  1.30118623e-01 -2.66220272e-01  2.01602221e-01\n",
            "  1.85664259e-02 -1.78889871e-01  4.21894481e-03  2.92499900e-01\n",
            " -3.63996953e-01  1.63290963e-01  1.68605730e-01 -1.33574501e-01\n",
            " -2.61724919e-01 -3.03645320e-02  4.14154455e-02  8.79094601e-02\n",
            " -2.66548723e-01  5.08435704e-02  3.36546570e-01 -1.66369319e-01\n",
            "  4.40177768e-01 -3.16583008e-01 -1.58571769e-02 -2.18596324e-01\n",
            "  9.59141329e-02  1.33635834e-01 -1.84853375e-01  2.39242256e-01\n",
            " -5.54350913e-02 -1.17268160e-01 -7.49406591e-03  1.70167461e-01\n",
            " -1.42037153e-01 -3.40194404e-02 -7.88212381e-03 -5.08738875e-01\n",
            " -2.98844337e-01 -1.50419399e-03  2.10880265e-01 -1.57367066e-01\n",
            " -1.49846792e-01  3.30751210e-01 -4.22234163e-02 -1.50460273e-01\n",
            " -3.38750221e-02 -2.38254994e-01 -6.23950288e-02 -2.40322184e-02\n",
            "  1.60175353e-01  4.00026739e-02 -1.74101606e-01  1.63567036e-01\n",
            "  4.92065474e-02  8.00767075e-03  3.32063973e-01  1.09715872e-01\n",
            " -2.62252927e-01 -1.11295588e-01  3.66825998e-01  4.65768516e-01\n",
            " -1.88092798e-01  7.54430071e-02  2.08515659e-01 -1.30864680e-01\n",
            "  1.23451855e-02 -4.65434510e-03  1.15341038e-01 -1.99946925e-01\n",
            " -2.46331349e-01 -6.74877539e-02  2.36696437e-01 -3.73025946e-02\n",
            "  1.18147343e-01 -2.50578344e-01 -1.23965383e-01  1.26402304e-01\n",
            " -1.83394864e-01  2.52055585e-01 -4.11232226e-02 -1.76565319e-01\n",
            "  7.45093673e-02  3.75078589e-01  1.68039292e-01  3.33875716e-01\n",
            " -1.58209242e-02  2.21204326e-01 -1.10281751e-01  2.48390753e-02\n",
            " -2.46719509e-01 -4.67490889e-02  4.71889935e-02 -2.82180011e-01\n",
            " -1.41586706e-01 -1.67016640e-01  2.04633236e-01 -2.31546640e-01\n",
            "  3.65937740e-01 -6.50668815e-02  1.28979132e-01 -1.46497712e-01\n",
            " -2.43296489e-01 -3.12187463e-01  7.70514831e-02  1.73691228e-01\n",
            "  1.33896098e-01  3.05127203e-01  8.97944272e-02  3.61582309e-01\n",
            " -2.30998576e-01  3.12777072e-01  2.17253864e-01 -1.87811852e-02\n",
            " -1.33316085e-01  2.60732114e-01  1.52453870e-01  1.39085189e-01\n",
            "  4.27427627e-02 -4.45636839e-01 -3.49123329e-01 -1.15854889e-01\n",
            "  9.39930826e-02 -5.87061420e-02 -2.36164294e-02  2.51463532e-01\n",
            " -2.22042520e-02 -1.57699034e-01  1.11639164e-02  1.26242027e-01\n",
            " -3.16527188e-01  1.05668634e-01  7.43401200e-02  8.60434622e-02\n",
            "  9.67203975e-02  1.63182184e-01  5.65271638e-02  1.74177289e-01\n",
            " -8.97223502e-03 -2.07908720e-01  1.81814313e-01  3.06922346e-01\n",
            " -9.08526331e-02  5.07829785e-01  6.99708313e-02 -3.58265549e-01\n",
            " -5.96489199e-03 -3.61274481e-01  1.87189803e-01 -1.12093039e-01\n",
            "  1.33485988e-01  5.67941256e-02  6.01415694e-01  3.21285039e-01\n",
            "  4.10073400e-02  8.14577565e-03  2.29330614e-01 -4.27159637e-01\n",
            " -3.24711680e-01  2.87422221e-02 -4.97408837e-01 -3.24877083e-01\n",
            " -1.27690479e-01  3.23051028e-02  2.22281083e-01  2.90840790e-02\n",
            " -1.89606890e-01 -4.32894006e-03  2.95275360e-01  1.20915703e-01\n",
            " -3.41107138e-04  1.31963775e-01  4.04338166e-02 -1.50196701e-01\n",
            " -3.07783693e-01 -3.66524905e-02 -3.86714041e-02  1.42627031e-01\n",
            " -3.98158059e-02 -1.45127252e-01 -7.38056302e-02  3.84623967e-02\n",
            " -1.41120717e-01  7.22300028e-04  2.49065701e-02  3.58907878e-01\n",
            "  9.24603194e-02 -1.70762405e-01 -1.49575740e-01  9.14777219e-02\n",
            " -6.28045667e-03  1.46612480e-01  7.87820816e-02  2.34607026e-01\n",
            "  1.19644195e-01 -2.10258394e-01  2.00300619e-01  5.10688312e-02\n",
            "  1.63731799e-02 -4.93166186e-02 -1.17124781e-01 -1.64759442e-01\n",
            "  1.79820359e-02 -1.79412678e-01  4.06947583e-02  2.12960199e-01\n",
            "  3.27019915e-02 -2.46986330e-01 -2.55603313e-01 -1.98646337e-01\n",
            "  1.60563380e-01  1.23918064e-01 -1.38947189e-01  2.44644880e-01\n",
            " -1.13524556e-01  3.55214663e-02 -2.48459429e-01  8.24358389e-02\n",
            " -1.07400119e-01 -1.86158568e-01 -3.19356143e-01  2.65429094e-02\n",
            "  3.24914336e-01  5.68624064e-02  7.11639449e-02 -6.91676289e-02\n",
            "  1.98311642e-01 -1.63472846e-01  2.52854705e-01 -1.09032989e-01\n",
            "  2.36313954e-01 -1.09589919e-01 -1.54916629e-01  1.11463703e-01\n",
            " -2.46308535e-01 -5.68784736e-02  8.59949924e-03  6.67734221e-02\n",
            "  8.38818923e-02 -3.86494190e-01 -1.63090259e-01  8.79987404e-02\n",
            "  1.02965087e-01 -1.89678639e-01 -1.70057863e-01  6.38653710e-02\n",
            "  3.74744311e-02 -3.09943378e-01  9.70577896e-02 -6.42170534e-02\n",
            " -3.03907752e-01  2.62816865e-02  1.64552346e-01 -1.08689122e-01\n",
            "  2.36671969e-01 -1.99395701e-01 -5.91916591e-02 -8.23433995e-02\n",
            " -1.54992908e-01 -5.55762649e-02  1.21878572e-01  1.95026949e-01\n",
            " -1.34722833e-02 -1.19822733e-01 -7.84737319e-02 -6.48662969e-02\n",
            " -1.03429258e-01 -7.44164735e-02 -1.05937183e-01  8.08179751e-03\n",
            " -1.78594142e-02  9.61910412e-02  2.44971484e-01  1.67631343e-01\n",
            "  2.16198504e-01  1.59258962e-01  1.36282638e-01 -4.60338555e-02\n",
            "  1.28291950e-01  3.07077914e-01 -2.30532572e-01  2.31822088e-01\n",
            " -2.79053867e-01  1.87639073e-01 -2.64102459e-01 -5.35147600e-02\n",
            " -1.02063850e-01  1.50199950e-01 -2.39779517e-01  2.88818598e-01\n",
            "  8.16448107e-02  3.96554142e-01 -1.20202273e-01  3.14988792e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIkTDKRiNKZG"
      },
      "source": [
        "dataset_rss = np.zeros((4001,256))\n",
        "J = np.arange(256)*2\n",
        "\n",
        "k=0\n",
        "for d in dataset:\n",
        "  i = 0\n",
        "  rss = np.zeros((1,256))\n",
        "\n",
        "  for l in J:\n",
        "      pr = d[l]*d[l]\n",
        "      #  + d[l+1]*d[l+1] + d[l+2]*d[l+2] + d[l+3]*d[l+3] \n",
        "      # print(pr)\n",
        "      # pr = pr*10e3\n",
        "      # if pr != 0:\n",
        "      if pr == 0:\n",
        "        rss[0,i] = 1\n",
        "      else:\n",
        "        rss[0,i]= (10*(math.log(pr)))/(-2000)\n",
        "      # else:\n",
        "      #   rss[0,i]=0\n",
        "      # print(rss[0,i])\n",
        "      i += 1\n",
        "      \n",
        "  dataset_rss[k,:] = rss\n",
        "  i = i+1\n",
        "  k += 1\n",
        "\n",
        "# max_rss = np.amax(dataset_rss)\n",
        "# min_rss = np.amin(dataset_rss)\n",
        "\n",
        "# i = 0\n",
        "# for d in dataset_rss:\n",
        "\n",
        "#   dataset_rss[i,0] = (2/(max_rss-min_rss))*(d-((max_rss+min_rss)/2))\n",
        "#   i = i+1\n",
        "\n",
        "# print(dataset_rss[18,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifGwYPW22dIn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "5299bcc8-91f0-480c-dd23-f4d0a4b0b799"
      },
      "source": [
        "print(dataset_rss[9,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.039112   0.01022716 0.05451472 0.01745655 0.02014099 0.0260527\n",
            " 0.02606163 0.02954045 0.0128527  0.03429485 0.03384636 0.00643988\n",
            " 0.03673875 0.0143484  0.01502324 0.02037055 0.02197475 0.00576273\n",
            " 0.02288828 0.01124134 0.03073906 0.00626778 0.0094807  0.01454538\n",
            " 0.02541982 0.00619773 0.00993339 0.04107165 0.04162972 0.01907112\n",
            " 0.01880811 0.01403771 0.03200862 0.02354816 0.02448593 0.01724095\n",
            " 0.05548541 0.01573436 0.00987135 0.02788049 0.01708262 0.00764552\n",
            " 0.02378347 0.01080655 0.03903362 0.01899661 0.02596317 0.03062469\n",
            " 0.00424805 0.03928387 0.01225695 0.02084598 0.0724936  0.02367258\n",
            " 0.00652395 0.01523814 0.01673035 0.027199   0.01841783 0.02040681\n",
            " 0.02387561 0.01976542 0.02330169 0.02005344 0.01935048 0.01376361\n",
            " 0.01992841 0.02508847 0.03602202 0.04230216 0.01341548 0.00547569\n",
            " 0.04664011 0.02023803 0.03631825 0.0147816  0.01150644 0.02374338\n",
            " 0.02253861 0.01379449 0.01050133 0.00728284 0.0299584  0.04271672\n",
            " 0.03944405 0.00781275 0.01380972 0.00604182 0.01916204 0.00536218\n",
            " 0.02934879 0.04950224 0.01403462 0.03011623 0.01740746 0.02167485\n",
            " 0.01111736 0.01355333 0.02062478 0.01936448 0.00537678 0.02793021\n",
            " 0.01621826 0.0237366  0.01026406 0.01700627 0.08115705 0.01886953\n",
            " 0.01906397 0.01835997 0.01311186 0.02083177 0.02159948 0.00965299\n",
            " 0.04273711 0.01353059 0.01715496 0.0134653  0.01653223 0.04696064\n",
            " 0.00491516 0.02133128 0.0267977  0.01551074 0.01637537 0.04005977\n",
            " 0.01350716 0.03340654 0.01172011 0.03273851 0.02964698 0.0208263\n",
            " 0.01657161 0.01619327 0.01398347 0.04859336 0.02734176 0.01534407\n",
            " 0.03367211 0.01361044 0.02710583 0.01818135 0.03858737 0.02967991\n",
            " 0.01640529 0.0175802  0.01066959 0.01632445 0.0073369  0.00895479\n",
            " 0.00896662 0.02300364 0.03012351 0.02171999 0.01224118 0.02522151\n",
            " 0.03263152 0.02068148 0.02347684 0.01831996 0.00975566 0.01038359\n",
            " 0.01392932 0.05882446 0.01633435 0.00526546 0.03221075 0.02399168\n",
            " 0.03219088 0.0326466  0.01498943 0.02386745 0.01713115 0.019258\n",
            " 0.0159355  0.01205942 0.01711303 0.01468219 0.01779682 0.04644868\n",
            " 0.02914955 0.03488626 0.01002325 0.02602601 0.02386927 0.01446773\n",
            " 0.01728402 0.01921914 0.00624871 0.02334953 0.02324698 0.0257561\n",
            " 0.02762889 0.02414296 0.01355956 0.01802176 0.02369952 0.01034125\n",
            " 0.00775152 0.01394528 0.01698517 0.01540745 0.01267619 0.01335977\n",
            " 0.02823126 0.01328437 0.01462911 0.02275952 0.01796012 0.01464392\n",
            " 0.03691921 0.03272184 0.01406094 0.01939774 0.03650779 0.02094003\n",
            " 0.01899102 0.02084548 0.02569098 0.01830347 0.03202208 0.01467767\n",
            " 0.0429345  0.03305372 0.02159041 0.01434853 0.01958803 0.02362487\n",
            " 0.02123708 0.00933948 0.03786192 0.02260847 0.01625498 0.01404919\n",
            " 0.01998295 0.0536233  0.01991861 0.01698543 0.02230725 0.01341682\n",
            " 0.01200472 0.02293245 0.02175868 0.00101332 0.03523452 0.04153733\n",
            " 0.04208378 0.01103068 0.01529662 0.01728693 0.04272803 0.01502856\n",
            " 0.01639594 0.01052174 0.01394662 0.0254663 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TDgVnhQQBkL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "outputId": "b8517a2b-efdb-434c-f183-576d59105d92"
      },
      "source": [
        "# creating data adaptive matrix:\n",
        "data_adaptive_dataset = np.zeros((4001,16,16,8))\n",
        "data_adaptive_labels = np.zeros((1,4001))\n",
        "data_template = np.zeros((16,16,8))\n",
        "\n",
        "step = 0\n",
        "i = 0\n",
        "d_num = 0\n",
        "\n",
        "\n",
        "for d in dataset_rss:\n",
        "  # print(d)\n",
        "\n",
        "  if step < 7:\n",
        "\n",
        "    if labels[i] == 1:\n",
        "      data_template[:,:,step] = np.reshape(d,(16,16))\n",
        "      # print(np.reshape(d,(16,16)))\n",
        "      step += 1\n",
        "  \n",
        "  else:\n",
        "\n",
        "    if labels[i] == 1:\n",
        "        data_template[:,:,step] = np.reshape(d,(16,16))\n",
        "        data_adaptive_labels[0,d_num] = 1\n",
        "        data_adaptive_dataset[d_num,:,:,:] = data_template\n",
        "        for i in range(7):\n",
        "          data_template[:,:,i] = data_template[:,:,i+1]\n",
        "        d_num += 1\n",
        "    else:\n",
        "      data_template[:,:,step] = np.reshape(d,(16,16))\n",
        "      data_adaptive_labels[0,d_num] = 0\n",
        "      data_adaptive_dataset[d_num,:,:,:] = data_template\n",
        "      d_num += 1\n",
        "  i += 1\n",
        "\n",
        "print(data_adaptive_dataset[398,:,:,:])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0.0400408  0.02446033 0.02234477 ... 0.032144   0.0170221  0.02017992]\n",
            "  [0.02784211 0.03337    0.01925792 ... 0.0361967  0.01751341 0.02260962]\n",
            "  [0.01832167 0.0137634  0.02528159 ... 0.02979073 0.0365043  0.00946362]\n",
            "  ...\n",
            "  [0.01883879 0.02172656 0.02731569 ... 0.02264612 0.02210473 0.01274514]\n",
            "  [0.02695309 0.02792797 0.02158767 ... 0.02306969 0.05952512 0.02193118]\n",
            "  [0.03104241 0.02507616 0.01156022 ... 0.01396794 0.02641282 0.01528252]]\n",
            "\n",
            " [[0.03589172 0.0284419  0.03500288 ... 0.02850294 0.02189964 0.01870794]\n",
            "  [0.05517382 0.02002929 0.02849669 ... 0.02373062 0.03149001 0.03663453]\n",
            "  [0.02303664 0.04435441 0.0233644  ... 0.01368134 0.02484315 0.05243771]\n",
            "  ...\n",
            "  [0.03568708 0.0110658  0.02409884 ... 0.01824183 0.02607307 0.01871167]\n",
            "  [0.04344098 0.0168579  0.02018782 ... 0.03277878 0.02315513 0.0243982 ]\n",
            "  [0.02109634 0.02004065 0.03113882 ... 0.01982085 0.01610486 0.01850356]]\n",
            "\n",
            " [[0.02657514 0.0357675  0.01966016 ... 0.024191   0.05691338 0.02937678]\n",
            "  [0.02355411 0.0221586  0.01409986 ... 0.01986859 0.030119   0.02202122]\n",
            "  [0.01786602 0.03137485 0.01222081 ... 0.02194998 0.01799459 0.01987036]\n",
            "  ...\n",
            "  [0.01565694 0.04515759 0.03225884 ... 0.0181215  0.01787286 0.02177734]\n",
            "  [0.02869307 0.03292888 0.02347278 ... 0.03098023 0.02337411 0.04363877]\n",
            "  [0.02198593 0.03273653 0.03001884 ... 0.02366645 0.01497435 0.01179598]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.04241375 0.02077938 0.02009152 ... 0.02134675 0.02903959 0.012651  ]\n",
            "  [0.02389995 0.02223276 0.01874995 ... 0.02974795 0.02584687 0.01669167]\n",
            "  [0.02722809 0.03738713 0.03495251 ... 0.02862458 0.03050552 0.01092945]\n",
            "  ...\n",
            "  [0.02685555 0.01578896 0.01724418 ... 0.03698619 0.03395209 0.00676193]\n",
            "  [0.01194934 0.02241397 0.02539062 ... 0.01623004 0.01472811 0.01885051]\n",
            "  [0.01856873 0.02474098 0.02337191 ... 0.02808127 0.02137854 0.02586324]]\n",
            "\n",
            " [[0.01428951 0.02272239 0.01021097 ... 0.02676444 0.0153968  0.02606059]\n",
            "  [0.02191883 0.01907203 0.02336765 ... 0.0179784  0.02603397 0.0214173 ]\n",
            "  [0.01537992 0.01228396 0.04628916 ... 0.03142654 0.02748291 0.04687073]\n",
            "  ...\n",
            "  [0.0260246  0.0220508  0.02722866 ... 0.04536741 0.03209826 0.02110479]\n",
            "  [0.06182215 0.02251932 0.02164377 ... 0.0226579  0.02130864 0.01702127]\n",
            "  [0.02792236 0.01993015 0.02082596 ... 0.03900819 0.01999833 0.0265607 ]]\n",
            "\n",
            " [[0.01488015 0.01397482 0.01284261 ... 0.01809149 0.02369666 0.04920678]\n",
            "  [0.04508651 0.02818192 0.04709511 ... 0.01611901 0.02560579 0.01155278]\n",
            "  [0.01796498 0.01531268 0.02461436 ... 0.0170855  0.01673199 0.02112596]\n",
            "  ...\n",
            "  [0.02424805 0.01734171 0.01479866 ... 0.05714667 0.02967526 0.01136031]\n",
            "  [0.03322116 0.02220441 0.01741508 ... 0.01836633 0.01491206 0.02689376]\n",
            "  [0.0245138  0.02237102 0.01720798 ... 0.02446679 0.02650637 0.02522891]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yqnO3AFy0sR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "outputId": "b53dbc5b-e3dc-472d-809e-81bb53c638aa"
      },
      "source": [
        "print(data_adaptive_dataset[0,:,:,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0.02778431 0.02946246 0.01663636 ... 0.04964793 0.02259223 0.0191332 ]\n",
            "  [0.01897661 0.00859868 0.03806784 ... 0.02486264 0.04292954 0.0045471 ]\n",
            "  [0.02775439 0.02053696 0.02232647 ... 0.02620163 0.02701714 0.02136698]\n",
            "  ...\n",
            "  [0.0183076  0.01589125 0.02146732 ... 0.02701235 0.02236613 0.00766828]\n",
            "  [0.01665188 0.03582272 0.02543706 ... 0.01526778 0.07971612 0.04083452]\n",
            "  [0.025487   0.01165024 0.05344878 ... 0.01975011 0.01548129 0.02552766]]\n",
            "\n",
            " [[0.02627431 0.01675928 0.03100446 ... 0.02677371 0.01657233 0.03799906]\n",
            "  [0.01937835 0.0155872  0.0197737  ... 0.01559088 0.02570553 0.03003218]\n",
            "  [0.0241231  0.02669949 0.03035872 ... 0.05168506 0.02011504 0.01930977]\n",
            "  ...\n",
            "  [0.0199606  0.05254098 0.0240831  ... 0.03648471 0.0379295  0.02314153]\n",
            "  [0.02845037 0.02378288 0.03771228 ... 0.03562355 0.03021996 0.01850799]\n",
            "  [0.02077968 0.03888237 0.02552595 ... 0.02268913 0.03734291 0.05814588]]\n",
            "\n",
            " [[0.05467387 0.02265048 0.03706448 ... 0.02901012 0.02037537 0.04561723]\n",
            "  [0.02349942 0.02713412 0.02980397 ... 0.03405508 0.01919356 0.02585118]\n",
            "  [0.02923334 0.03446056 0.01317508 ... 0.02636545 0.01231982 0.01189109]\n",
            "  ...\n",
            "  [0.02212879 0.01917323 0.05801447 ... 0.06150018 0.01666185 0.02832787]\n",
            "  [0.02029372 0.02423396 0.02229546 ... 0.03315039 0.02892307 0.00692647]\n",
            "  [0.03213976 0.0162693  0.01265688 ... 0.02590956 0.02651499 0.0142439 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.02373432 0.0409331  0.0226132  ... 0.01988771 0.01736169 0.01917112]\n",
            "  [0.01991393 0.02112139 0.06185533 ... 0.0538297  0.02278582 0.01337306]\n",
            "  [0.01823255 0.03200732 0.02418776 ... 0.02738712 0.02551368 0.00541876]\n",
            "  ...\n",
            "  [0.02921153 0.02305517 0.01245904 ... 0.02071305 0.02341992 0.02439789]\n",
            "  [0.02173054 0.04292445 0.00963831 ... 0.05635391 0.01751649 0.02482357]\n",
            "  [0.03772706 0.02489663 0.01856316 ... 0.01480086 0.01662848 0.02607351]]\n",
            "\n",
            " [[0.03677568 0.01878637 0.03746096 ... 0.03265144 0.01784904 0.00976741]\n",
            "  [0.03060152 0.03739915 0.01622067 ... 0.01801057 0.03576798 0.01630438]\n",
            "  [0.01439552 0.04586822 0.01475945 ... 0.01414532 0.03865175 0.01017403]\n",
            "  ...\n",
            "  [0.02985313 0.04648879 0.02931359 ... 0.02080792 0.01589621 0.00783004]\n",
            "  [0.03399817 0.02260638 0.01826943 ... 0.02077301 0.02938305 0.01678961]\n",
            "  [0.03573453 0.02357643 0.03498129 ... 0.06558974 0.02884074 0.02343983]]\n",
            "\n",
            " [[0.03786904 0.03065528 0.02821501 ... 0.01977337 0.01256831 0.02733353]\n",
            "  [0.01814211 0.02395143 0.01790088 ... 0.01909581 0.02275553 0.01976896]\n",
            "  [0.01377221 0.02290772 0.03516296 ... 0.03821598 0.02330801 0.01289938]\n",
            "  ...\n",
            "  [0.03581406 0.03406105 0.02397064 ... 0.0238725  0.02113496 0.01151053]\n",
            "  [0.02378612 0.03265344 0.04600686 ... 0.03855891 0.064965   0.00746673]\n",
            "  [0.02399321 0.02164265 0.01651796 ... 0.01699219 0.02516659 0.02661896]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujG_T_TpBblN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97eb4cb7-50f3-47c8-8287-72e6db02616c"
      },
      "source": [
        "from keras.layers import MaxPooling2D as maxpool\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(16, 16, 8)))\n",
        "model.add(Conv2D(32, kernel_size=2, padding='valid', strides=(2, 2), activation='relu'))\n",
        "model.add(maxpool((2, 2), padding='valid', strides=(2,2)))\n",
        "model.add(Conv2D(128, kernel_size=2, padding='valid', strides=(2, 2), activation='relu'))\n",
        "# model.add(Dropout(0.25))\n",
        "model.add(maxpool((2, 2), padding='valid', strides=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=4000,\n",
        "    decay_rate=0.9)\n",
        "\n",
        "adam = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_data = data_adaptive_dataset[0:2000,:,:,:]\n",
        "train_labels = data_adaptive_labels[0,0:2000]\n",
        "history = model.fit(train_data, train_labels,\n",
        "                epochs=150,\n",
        "                batch_size=64,\n",
        "                validation_split=0.1)\n",
        "# print(train_data[1,:,:,:])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.5077 - accuracy: 0.9628 - val_loss: 0.1839 - val_accuracy: 1.0000\n",
            "Epoch 2/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0546 - accuracy: 0.9983 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
            "Epoch 3/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 4/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 5/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 6/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 7/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 0.9983 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 8/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 9/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 10/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 11/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 12/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 13/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 14/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 15/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 16/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 17/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 18/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 19/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 20/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 21/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 22/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 23/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 24/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 25/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 26/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 27/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 28/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 29/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 30/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 31/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 32/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 33/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 34/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 35/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 36/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 37/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 38/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 39/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 40/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 41/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 42/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 43/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 44/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 45/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 46/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 47/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 48/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 49/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 50/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 51/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 52/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 53/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 54/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 55/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 56/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 57/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 58/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 59/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 60/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 61/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 62/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 63/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 64/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 65/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 66/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 67/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 68/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 69/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.9983 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 70/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 71/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 72/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 73/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 74/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 75/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 76/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 77/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 78/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 79/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 80/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 81/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 82/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 83/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 84/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 85/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.9983 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 86/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0120 - accuracy: 0.9983 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 87/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 88/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 89/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 0.9983 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 90/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.9983 - val_loss: 8.5064e-04 - val_accuracy: 1.0000\n",
            "Epoch 91/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 92/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 93/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 94/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.9983 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 95/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0120 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 96/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 97/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 98/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 99/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.9983 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 100/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 101/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.9983 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 102/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0120 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 103/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 104/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 105/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 106/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 107/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.9983 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 108/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 109/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 110/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 111/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 112/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 113/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 0.9983 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 114/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 115/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 116/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 117/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.9983 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
            "Epoch 118/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 119/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 120/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 121/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 122/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0106 - accuracy: 0.9983 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 123/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 124/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 125/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 0.9983 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 126/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 127/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.9983 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 128/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 129/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 130/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 131/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 132/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 133/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0099 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 134/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.9983 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 135/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 136/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 137/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 138/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 139/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 140/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0092 - accuracy: 0.9983 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 141/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 0.9983 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 142/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 143/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 144/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 145/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 146/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 147/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 148/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 149/150\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 0.0084 - accuracy: 0.9983 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 150/150\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.0083 - accuracy: 0.9983 - val_loss: 0.0013 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG9udIKiiYG6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5e727017-51d1-4082-a4cd-547d6178d353"
      },
      "source": [
        "for layer in model.layers:\n",
        "    print(layer.output_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 8, 8, 32)\n",
            "(None, 4, 4, 32)\n",
            "(None, 2, 2, 128)\n",
            "(None, 2, 2, 128)\n",
            "(None, 1, 1, 128)\n",
            "(None, 128)\n",
            "(None, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bUYzOC_4179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "8e57c438-de59-40b9-c8a2-b49e9584828f"
      },
      "source": [
        "# print(history.)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='SNR=1')\n",
        "plt.xlabel('itteration')\n",
        "plt.ylabel('training loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fed59d3d4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}